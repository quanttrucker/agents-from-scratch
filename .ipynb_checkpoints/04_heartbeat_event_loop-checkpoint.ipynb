{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1112fe-d321-4e05-a1a6-71c3ee0957f2",
   "metadata": {},
   "source": [
    "<b>Memory management on validation failures (MemGPT)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4106a302-4a4c-4ead-a871-47e163249ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7702360e-ce96-4980-9e86-93464184b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede7a32-214d-4a0a-b878-0e855d7ce7c7",
   "metadata": {},
   "source": [
    "Stops treating the LLM as a chatbot and starts treating it as a CPU. In this paradigm, the context window is treated as finite RAM, and vector databases are treated as massive, slow Disk Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc5cc6-40d0-4d21-9298-1c0d211db613",
   "metadata": {},
   "source": [
    "1. <b>The Two-Tier Memory Architecture</b> <br>\n",
    "Instead of passing one giant list of messages to the LLM, the state is split into a strict hierarchy.\n",
    "\n",
    "<i>Tier 1: Core Memory (Main Context / RAM) </i> <br>\n",
    "\n",
    "This is the only data passed to the LLM's prompt on every single turn. It is kept intentionally tiny.\n",
    "\n",
    "It contains two primary text blocks: the Persona block (who the agent is and what it is doing) and the Human block (a living summary of the user and current state).\n",
    "\n",
    "Crucially: The agent has the power to actively edit these blocks itself. <br>\n",
    "\n",
    "<i>Tier 2: External Memory (Disk Storage) </i> <br>\n",
    "\n",
    "This data lives completely outside the LLM's context window.\n",
    "\n",
    "Recall Memory: A standard SQLite database containing the raw, first-in-first-out (FIFO) chat history and tool tracebacks.\n",
    "\n",
    "Archival Memory: A Vector Database (like pgvector, Milvus, or Chroma) used for semantic, long-term knowledge retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6f131-fe23-4062-8c74-984e38988e4d",
   "metadata": {},
   "source": [
    "2. <b>Mechanics of Pagination and Compression</b> <br>\n",
    "\n",
    "In a standard RAG system, retrieval is passive (a user asks a query, and the system shoves vectors into the prompt). <br> \n",
    "In the MemGPT architecture, retrieval and compression are active and agent-driven. <br>\n",
    "<i>Context Compression (Cognitive Triage) </i>: When the token count of the active conversation queue approaches a predefined limit (e.g., $N_{tokens} > 0.7 \\times C_{max}$), the system sends a \"Memory Pressure\" warning to the LLM.The LLM is forced to pause and execute a compression routine: it reads the oldest messages, extracts the vital semantic facts (\"User prefers Python\", \"DB schema has 5 tables\"), updates its Tier 1 Core Memory with those facts, and then flushes the raw messages out of the active context into Tier 2 Recall Memory. <br> \n",
    "<i>Pagination (Page Faults) </i>: If the LLM needs information that isn't in its Tier 1 Core Memory, it triggers a \"System Call\" (a tool execution) to page it in from Tier 2, much like an OS pulling a page from a swap file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba4aeb-d6cc-44f0-a6fe-03cbbfd782b3",
   "metadata": {},
   "source": [
    "3. <i>The \"System Calls\" (Python Toolset) </i> <br>\n",
    "To implement this, you give the LLM access to a specific suite of memory-management tools. Instead of just query_database, your TOOL_REGISTRY now includes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bf13e0-eb21-409e-a5f4-58c84b67ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier 1 Operations (RAM)\n",
    "def core_memory_append(block: str, content: str):\n",
    "    \"\"\"Agent appends a new fact to its active system prompt.\"\"\"\n",
    "    \n",
    "def core_memory_replace(block: str, old_content: str, new_content: str):\n",
    "    \"\"\"Agent updates an existing fact in its active system prompt.\"\"\"\n",
    "\n",
    "# Tier 2 Operations (Disk)\n",
    "def archival_memory_insert(content: str):\n",
    "    \"\"\"Agent decides a fact is important but not urgent, saving it to the Vector DB.\"\"\"\n",
    "\n",
    "def archival_memory_search(query: str, page: int = 0):\n",
    "    \"\"\"Agent searches the Vector DB. Returns a paginated chunk of results.\"\"\"\n",
    "\n",
    "def conversation_search(query: str, date_range: str):\n",
    "    \"\"\"Agent searches the flushed historical logs for a specific traceback or chat.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8d017-f9b3-4721-b378-4442b29494a5",
   "metadata": {},
   "source": [
    "<b>Notes: </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfb256-924c-4410-8ec6-ca481623d102",
   "metadata": {},
   "source": [
    "By giving the agent these tools, it becomes a self-editing state machine. If you tell it, \"I'm moving to London,\" it doesn't just leave that in the chat log to eventually get pushed out of the context window. It actively calls core_memory_replace(\"human\", \"User lives in NY\", \"User lives in London\"). The token footprint remains incredibly small, but the agent's knowledge becomes functionally infinite. <br>\n",
    "\n",
    "The trickiest part of implementing this is the Control Flow. When the LLM calls archival_memory_search, you don't want it to immediately return that raw data to the user; you want the LLM to read it, think about it, and then formulate a response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5fe2e-0cde-4bbe-9f5a-2a38accbca83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d019ecb-12ba-4687-a9fc-347b37ebdde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223f26a-3614-47e3-893d-8bb0ec0cefe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
