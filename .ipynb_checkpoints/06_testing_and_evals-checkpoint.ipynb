{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1112fe-d321-4e05-a1a6-71c3ee0957f2",
   "metadata": {},
   "source": [
    "<b>CI/CD Testing Pipelines</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4106a302-4a4c-4ead-a871-47e163249ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7702360e-ce96-4980-9e86-93464184b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede7a32-214d-4a0a-b878-0e855d7ce7c7",
   "metadata": {},
   "source": [
    "To build a CI/CD pipeline for a non-deterministic state machine, you have to stop testing the exact output strings and start testing the logical bounds of the agent's trajectory. In the industry, we solve this using Programmatic Evaluation (often implemented via frameworks like Ragas or TruLens) where we use a faster, cheaper, deterministic LLM (the \"Judge\") to score the trace of the main agent's execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc5cc6-40d0-4d21-9298-1c0d211db613",
   "metadata": {},
   "source": [
    "The Triad of Agentic Evaluation MetricsTo test a system that retrieves memory and reasons over it, you must evaluate three distinct mathematical properties of the execution trace.Let $Q$ be the User Query, $C$ be the Context retrieved from FAISS, and $A$ be the Agent's Final Answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab74813-5e07-4ab2-bb27-9e28c1e3ee57",
   "metadata": {},
   "source": [
    "<b>1. Context Relevance (Testing the Disk Drive) </b> <br>Before we care about what the LLM says, we must test if the vector database returned garbage. Context Relevance measures the signal-to-noise ratio of the retrieved memory blocks.<br>\n",
    "<i>The Math: </i> The Judge LLM analyzes the context $C$ and extracts the number of sentences that are strictly necessary to answer $Q$.$$\\text{Context Relevance} = \\frac{|\\text{Relevant Sentences in } C|}{|\\text{Total Sentences in } C|}$$ <br>\n",
    "<i>Why it matters for CI/CD: </i> If this score drops below 0.5 in your pipeline, your FAISS search tool is pulling in too much noise, which will inflate your context window and confuse the agent. You need to adjust your top_k or improve your embedding strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9cf230-5264-42b8-93cc-5aaf93f36d6f",
   "metadata": {},
   "source": [
    "<b>2. Faithfulness (Testing for Hallucination) </b> <br>\n",
    "This is the most critical metric. Faithfulness ensures the agent didn't invent facts outside of its Tier 1 Core Memory or its retrieved Tier 2 Archival Memory. <br>\n",
    "<i>The Math: </i> The Judge LLM breaks the agent's final answer $A$ into a set of atomic claims $\\{c_1, c_2, ..., c_n\\}$. It then checks if each claim $c_i$ can be logically inferred from the Context $C$.$$\\text{Faithfulness} = \\frac{|\\text{Claims logically supported by } C|}{|\\text{Total Claims in } A|}$$ <br>\n",
    "<i>Why it matters for CI/CD: </i> If Faithfulness is anything less than 1.0 (100%), the build fails. A score of 0.8 means the agent hallucinates 20% of its statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f3fe8-5da3-41c5-ae18-de7be9786bd9",
   "metadata": {},
   "source": [
    "<b>3. Answer Relevance (Testing the Final Output) </b> Did the agent actually answer the user's prompt, or did it get distracted by the context it retrieved? <br>\n",
    "<i> The Math: </i> This is often calculated using embedding distances. We use an embedding model to vectorize the final Answer $\\vec{A}$ and the original Query $\\vec{Q}$, and calculate their cosine similarity.$$\\text{Answer Relevance} = \\frac{\\vec{A} \\cdot \\vec{Q}}{||\\vec{A}|| \\times ||\\vec{Q}||}$$ <br>\n",
    "<i>Why it matters for CI/CD: </i> A highly faithful answer can still be completely irrelevant to the prompt. This ensures the agent's core instruction-following policy remains intact. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8144171-88f7-4aa5-afa0-f8153f97504e",
   "metadata": {},
   "source": [
    "<b> Implementing the CI/CD Pipeline </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89cb8e1-a307-4adb-8758-3bff019f9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# 1. The Golden Dataset (Your test cases)\n",
    "TEST_CASES = [\n",
    "    {\n",
    "        \"query\": \"What is the max database timeout?\",\n",
    "        \"expected_facts\": [\"timeout is 30 seconds\"],\n",
    "    },\n",
    "    # ... 100 more edge cases ...\n",
    "]\n",
    "\n",
    "def evaluate_trace_with_judge(query: str, retrieved_context: str, final_answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calls a fast, cheap Judge LLM (e.g., Claude 3.5 Haiku or GPT-4o-mini) \n",
    "    with a strict prompt to calculate the Triad metrics.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the Query: {query}\n",
    "    Given the Context: {retrieved_context}\n",
    "    Given the Answer: {final_answer}\n",
    "    \n",
    "    Calculate Faithfulness, Context Relevance, and Answer Relevance as floats between 0.0 and 1.0.\n",
    "    Return strictly as JSON.\n",
    "    \"\"\"\n",
    "    # ... call Judge LLM ...\n",
    "    return {\"faithfulness\": 1.0, \"context_relevance\": 0.8, \"answer_relevance\": 0.95}\n",
    "\n",
    "@pytest.mark.parametrize(\"test_case\", TEST_CASES)\n",
    "def test_agent_execution_trajectory(test_case):\n",
    "    # 1. Run the Agent (The Heartbeat Loop we built)\n",
    "    # We modify the run_agent function to return the full state_memory trace, not just the final string.\n",
    "    trace = run_os_agent(test_case[\"query\"])\n",
    "    \n",
    "    # 2. Extract variables from the trace\n",
    "    final_answer = trace[-1][\"content\"]\n",
    "    retrieved_context = extract_tool_results_from_trace(trace, tool_name=\"archival_memory_search\")\n",
    "    \n",
    "    # 3. Score the execution using the Judge LLM\n",
    "    scores = evaluate_trace_with_judge(test_case[\"query\"], retrieved_context, final_answer)\n",
    "    \n",
    "    # 4. The CI/CD Assertions\n",
    "    assert scores[\"faithfulness\"] >= 0.99, f\"Agent hallucinated! Score: {scores['faithfulness']}\"\n",
    "    assert scores[\"context_relevance\"] >= 0.50, f\"FAISS retrieved too much noise.\"\n",
    "    assert scores[\"answer_relevance\"] >= 0.85, f\"Agent evaded the question.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba4aeb-d6cc-44f0-a6fe-03cbbfd782b3",
   "metadata": {},
   "source": [
    "<b>Step 3: The Architecture in Action (Control Flow) </b><br>\n",
    "Let's look at how this plays out inside your heartbeat event loop from the previous step.\n",
    "\n",
    "User asks a hard question: \"What were the key takeaways from that 50-page technical spec I gave you last month?\"\n",
    "\n",
    "LLM Execution 1: The LLM realizes it doesn't know. It calls ArchivalMemorySearch(search_query=\"technical spec takeaways last month\", top_k=5).\n",
    "\n",
    "Python Execution: Your script runs the FAISS search, grabs the 5 closest text chunks, and formats them into a single string.\n",
    "\n",
    "Context Update: The python script appends {\"role\": \"system\", \"content\": \"Tool Result: [Match 1] The spec emphasized... [Match 2]...\"} to the state_memory list.\n",
    "\n",
    "The Heartbeat: Because the tool requested a heartbeat, the python script does not wait for the user. It immediately loops and passes the newly updated context window back to the LLM.\n",
    "\n",
    "LLM Execution 2: The LLM reads the system prompt containing the retrieved text, synthesizes the answer, and finally calls SendMessage(message=\"Based on the spec, the key takeaways were...\").\n",
    "\n",
    "Pause: The agent sets request_heartbeat = False and the thread goes to sleep, waiting for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500aee3-f794-4ed4-920d-7e26870f565c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d019ecb-12ba-4687-a9fc-347b37ebdde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223f26a-3614-47e3-893d-8bb0ec0cefe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
