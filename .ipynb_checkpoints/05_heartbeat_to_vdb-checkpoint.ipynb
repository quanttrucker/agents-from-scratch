{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1112fe-d321-4e05-a1a6-71c3ee0957f2",
   "metadata": {},
   "source": [
    "<b>Stateful Agent with Infinite Recall</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4106a302-4a4c-4ead-a871-47e163249ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7702360e-ce96-4980-9e86-93464184b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede7a32-214d-4a0a-b878-0e855d7ce7c7",
   "metadata": {},
   "source": [
    "If we continue the OS metaphor, the agent's context window is RAM, and the Vector Database is the Hard Drive. But unlike a standard SQL database where you search by exact keywords, a Vector Database allows the agent to search by concept and meaning. We now need to integrate a local vector database (e.g. Meta's FAISS or Postgres pgvector) into your agent's toolset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc5cc6-40d0-4d21-9298-1c0d211db613",
   "metadata": {},
   "source": [
    "<b>The Mathematical Foundation: How the Agent \"Reads\" Disk </b><br> \n",
    "Before writing the tools you must understand what happens when the agent pages memory.When the agent executes a search for \"customer refund policy\", that text string is passed through an embedding model (like text-embedding-3-small) which converts it into a high-dimensional dense vector $\\vec{q} \\in \\mathbb{R}^n$ (where $n$ is typically 1536).<br>\n",
    "The Vector DB stores all your past documents as vectors $\\vec{v}_i$. To find the most relevant memory, FAISS calculates the Euclidean distance (L2 norm) between the agent's query vector and every stored vector:$$d(\\vec{q}, \\vec{v}_i) = ||\\vec{q} - \\vec{v}_i||_2 = \\sqrt{\\sum_{j=1}^{n} (q_j - v_{ij})^2}$$The database returns the top $K$ vectors with the smallest distance, allowing the agent to instantly retrieve semantically related facts even if the exact keywords don't match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab74813-5e07-4ab2-bb27-9e28c1e3ee57",
   "metadata": {},
   "source": [
    "<b>Step 1: Initialize the \"Disk Drive\" </b> <br>\n",
    "First, we set up FAISS and a metadata store. FAISS only stores the math (the vectors); we need a simple Python list or dictionary to store the actual text that maps to those vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d90345-ba33-4402-ab07-36d84d7974ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Define the Vector Space\u001b[39;00m\n\u001b[0;32m      5\u001b[0m VECTOR_DIMENSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1536\u001b[39m \u001b[38;5;66;03m# Matches the embedding model's output size\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# 1. Define the Vector Space\n",
    "VECTOR_DIMENSION = 1536 # Matches the embedding model's output size\n",
    "disk_index = faiss.IndexFlatL2(VECTOR_DIMENSION) \n",
    "\n",
    "# 2. Define the Metadata Store (maps vector IDs to original text)\n",
    "disk_metadata = []\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulates calling an embedding API (OpenAI, HuggingFace, etc.)\n",
    "    Returns a 1D numpy array of float32, length 1536.\n",
    "    \"\"\"\n",
    "    # ... standard API call to get vector ...\n",
    "    return np.random.rand(VECTOR_DIMENSION).astype('float32') # Mock vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6f131-fe23-4062-8c74-984e38988e4d",
   "metadata": {},
   "source": [
    "<b>Step 2: Build the Agent's Memory Tools </b><br>\n",
    "Now, we create the specific tools the agent will use during its heartbeat loop. Notice how we enforce request_heartbeat = True on the search tool. This ensures that after the agent fetches the data, it is immediately forced to read it rather than waiting for the user to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89cb8e1-a307-4adb-8758-3bff019f9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- TOOL SCHEMA DEFINITIONS ---\n",
    "\n",
    "class ArchivalMemoryInsert(BaseModel):\n",
    "    request_heartbeat: bool = Field(default=True)\n",
    "    fact_text: str = Field(description=\"The detailed fact or document to save to long-term memory.\")\n",
    "\n",
    "class ArchivalMemorySearch(BaseModel):\n",
    "    request_heartbeat: bool = Field(default=True) # Force the agent to read the result!\n",
    "    search_query: str = Field(description=\"The semantic concept you want to search your memory for.\")\n",
    "    top_k: int = Field(default=3, description=\"Number of memories to page into context.\")\n",
    "\n",
    "# --- TOOL IMPLEMENTATIONS ---\n",
    "\n",
    "def archival_memory_insert(args: ArchivalMemoryInsert) -> str:\n",
    "    \"\"\"Writes a new fact to the Vector DB.\"\"\"\n",
    "    # 1. Convert text to math\n",
    "    vector = get_embedding(args.fact_text)\n",
    "    \n",
    "    # 2. Reshape for FAISS (requires a 2D matrix, even for 1 vector)\n",
    "    vector_matrix = np.array([vector])\n",
    "    \n",
    "    # 3. Store the math in FAISS, store the text in our metadata list\n",
    "    disk_index.add(vector_matrix)\n",
    "    disk_metadata.append(args.fact_text)\n",
    "    \n",
    "    return f\"Successfully saved to archival storage. Memory ID: {len(disk_metadata) - 1}\"\n",
    "\n",
    "def archival_memory_search(args: ArchivalMemorySearch) -> str:\n",
    "    \"\"\"Pages facts from the Vector DB back into the agent's active context.\"\"\"\n",
    "    if disk_index.ntotal == 0:\n",
    "        return \"Archival memory is currently empty.\"\n",
    "        \n",
    "    # 1. Convert query to math\n",
    "    query_vector = get_embedding(args.search_query)\n",
    "    query_matrix = np.array([query_vector])\n",
    "    \n",
    "    # 2. Perform the Mathematical Search\n",
    "    distances, indices = disk_index.search(query_matrix, args.top_k)\n",
    "    \n",
    "    # 3. Format the results for the LLM to read\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx != -1: # FAISS returns -1 if there aren't enough vectors\n",
    "            retrieved_text = disk_metadata[idx]\n",
    "            score = distances[0][i]\n",
    "            results.append(f\"[Match {i+1} | Distance: {score:.4f}] {retrieved_text}\")\n",
    "            \n",
    "    if not results:\n",
    "        return \"No relevant memories found.\"\n",
    "        \n",
    "    # Return a massive string block. The heartbeat loop appends this to the context window.\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba4aeb-d6cc-44f0-a6fe-03cbbfd782b3",
   "metadata": {},
   "source": [
    "<b>Step 3: The Architecture in Action (Control Flow) </b><br>\n",
    "Let's look at how this plays out inside your heartbeat event loop from the previous step.\n",
    "\n",
    "User asks a hard question: \"What were the key takeaways from that 50-page technical spec I gave you last month?\"\n",
    "\n",
    "LLM Execution 1: The LLM realizes it doesn't know. It calls ArchivalMemorySearch(search_query=\"technical spec takeaways last month\", top_k=5).\n",
    "\n",
    "Python Execution: Your script runs the FAISS search, grabs the 5 closest text chunks, and formats them into a single string.\n",
    "\n",
    "Context Update: The python script appends {\"role\": \"system\", \"content\": \"Tool Result: [Match 1] The spec emphasized... [Match 2]...\"} to the state_memory list.\n",
    "\n",
    "The Heartbeat: Because the tool requested a heartbeat, the python script does not wait for the user. It immediately loops and passes the newly updated context window back to the LLM.\n",
    "\n",
    "LLM Execution 2: The LLM reads the system prompt containing the retrieved text, synthesizes the answer, and finally calls SendMessage(message=\"Based on the spec, the key takeaways were...\").\n",
    "\n",
    "Pause: The agent sets request_heartbeat = False and the thread goes to sleep, waiting for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500aee3-f794-4ed4-920d-7e26870f565c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d019ecb-12ba-4687-a9fc-347b37ebdde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223f26a-3614-47e3-893d-8bb0ec0cefe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
